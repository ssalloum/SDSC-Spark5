{"cells":[{"cell_type":"markdown","metadata":{"id":"ICXbyhtyjXZM"},"source":["#**Spark Structured Streaming: DataFrames and Queries**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LlkdCTePkOd"},"outputs":[],"source":["# Clone the GitHub repository\n","!git clone https://github.com/ssalloum/SDSC-Spark5.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDt-vQnbPqi1"},"outputs":[],"source":["!ls /content/SDSC-Spark5/data"]},{"cell_type":"markdown","source":["# Install Spark and Create SparkSession\n"],"metadata":{"id":"TKaEGJ6S3-6n"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"id":"D9_7rLRI4D36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder\\\n","        .master(\"local[*]\")\\\n","        .appName(\"Streaming Aggregations\")\\\n","        .getOrCreate()\n","\n","spark"],"metadata":{"id":"vlvFSjQ_4FMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuFT9utlPDOz"},"source":["# Structured Streaming"]},{"cell_type":"markdown","metadata":{"id":"edjHDff-PGPD"},"source":["* Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.\n","* You can express your streaming computation the same way you would express a batch computation on static data.\n","* The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.\n","* [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n","* [Structured Streaming API Reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html): pyspark.sql.streaming"]},{"cell_type":"markdown","metadata":{"id":"SKXVA-zAArdz"},"source":["## Core Classes\n","* [pyspark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html)\n","* [pyspark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html)\n","* [pyspark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html)\n","* [pyspark.sql.streaming.StreamingQueryManager](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryManager.html)\n","* [pyspark.sql.streaming.StreamingQueryListener](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryListener.html)"]},{"cell_type":"markdown","metadata":{"id":"dvt7tK1CBqK1"},"source":["# Data: Heterogeneity Human Activity Recognition Dataset\n","* In this notebook, we will work with the [Heterogeneity Human Activity Recognition Dataset](https://archive.ics.uci.edu/dataset/344/heterogeneity+activity+recognition).\n","* This data is also avaiable in the [Spark: The Definitive Guide](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data) Github repository, in the activity data folder.\n","* The data consists of smartphone and smartwatch sensor readings from a variety of devices.\n","* Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking, and so on.\n","* There are several different smartphones and smartwatches used, and nine total users.\n"]},{"cell_type":"markdown","source":["## Create a Static DataFrame"],"metadata":{"id":"Q6lcZ7rw49Rm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oP86sYkICzbu"},"outputs":[],"source":["sensorDataPath = \"/content/SDSC-Spark5/data/activity-data\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPHnRFcOCcwn"},"outputs":[],"source":["sensorStatic = spark.read.json(sensorDataPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"133D6oUvESt_"},"outputs":[],"source":["sensorStatic.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlLCoAUpjFaY"},"outputs":[],"source":["sensorStatic.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ms7-Ywg-m_kJ"},"outputs":[],"source":["sensorStatic.groupBy(\"gt\").count().show()"]},{"cell_type":"markdown","metadata":{"id":"Wzssk1V8hFRL"},"source":["# Streaming Queries\n","* The easiest way to get started with Structured Streaming is to use files as streaming source.\n"]},{"cell_type":"markdown","metadata":{"id":"PIWTmfHe1WsN"},"source":["## Define Input Source\n","* [pyspark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html)\n","* [pyspark.sql.SparkSession.readStream](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.readStream.html)\n","* `maxFilesPerTrigger`: file processing rate (maximum number of new files to read in every batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciVNMIsWklby"},"outputs":[],"source":["#create a streaming dataframe from json files\n","sensorStreaming = spark.readStream.schema(sensorStatic.schema).option(\"maxFilesPerTrigger\", 5)\\\n","                                    .json(sensorDataPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wg0UqUOEQFd"},"outputs":[],"source":["sensorStreaming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KUOHaVITJVD"},"outputs":[],"source":["sensorStreaming.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsnVlLYqToE5"},"outputs":[],"source":["sensorStreaming.count()"]},{"cell_type":"markdown","metadata":{"id":"D3kiGEnfVhVx"},"source":["* [pyspark.sql.DataFrame.isStreaming](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.isStreaming.html): check whether a DataFrame is a streaming DataFrame or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6MiO79wVZr3"},"outputs":[],"source":["sensorStreaming.isStreaming"]},{"cell_type":"markdown","metadata":{"id":"aCJPY8452C3i"},"source":["## Define Transformations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swCXwcGblq66"},"outputs":[],"source":["# count the number of rows for each activity type\n","activityCounts = sensorStreaming.groupBy(\"gt\").count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ToMDoQTcmCaM"},"outputs":[],"source":["activityCounts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jSPpB2tVJjX"},"outputs":[],"source":["activityCounts.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hL7elrAqWK8w"},"outputs":[],"source":["activityCounts.isStreaming"]},{"cell_type":"markdown","metadata":{"id":"zIKt2UGP2OPY"},"source":["## Specify Output Sink, Output Mode and Processing Details\n","* [pyspark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html)\n","* [pyspark.sql.DataFrame.writeStream](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeStream.html)\n","* In this example, we use the Memory sink so that the output is stored in memory as an in-memory table.  This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driverâ€™s memory. Also, Memory sink is not fault-tolerant because it does not guarantee persistence of the output and is meant for debugging purposes only.\n","* The query name will be used as the table name.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYl8qQy2liK7"},"outputs":[],"source":["activityWriter = activityCounts.writeStream.queryName(\"activityCounts\")\\\n","  .format(\"memory\")\\\n","  .outputMode(\"complete\")\\\n","  .trigger(processingTime=\"1 second\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_yylNzNpusl"},"outputs":[],"source":["activityWriter"]},{"cell_type":"markdown","metadata":{"id":"lzcVZsCylWyq"},"source":["## Start the Streaming Query\n","* [pyspark.sql.streaming.DataStreamWriter.start](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.start.html)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_99i4DCf3NOr"},"outputs":[],"source":["activityQuery = activityWriter.start()\n","activityQuery"]},{"cell_type":"markdown","metadata":{"id":"ykPVqVzAvSM9"},"source":["* [pyspark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html)\n","* `activityQuery` is a handle to the streaming query  named `activityCounts` that is running in the background. It can be used to manage and monitor the query as we will see later.\n","* In this example, this query continuously picks up files and updates the result counts.\n","* `start()` is a nonblocking method, so it will return as soon as the query has started in the background.\n","* If you want the main thread to block until the streaming query has terminated, you can use [StreamingQuery.awaitTermination()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.awaitTermination.html) (this is necessary in a production environment)."]},{"cell_type":"markdown","metadata":{"id":"DCK07ykclbOL"},"source":["## Query the Result Table\n","While the streaming query is running, we can query the result table (it has the same name as the query)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceQjMvs9uSm_"},"outputs":[],"source":["spark.sql(\" select * from activityCounts\").show()"]},{"cell_type":"markdown","source":["We can also user spark.table() tog et the result table as a DataFrame."],"metadata":{"id":"bgA9qIIgNnzG"}},{"cell_type":"code","source":["spark.table(\"activityCounts\").show()"],"metadata":{"id":"KIEq7BPuNzNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVlwBFMqnjDb"},"outputs":[],"source":["#we can also define a simple loop that will print the results every certain interval (1 second for example)\n","from time import sleep\n","for x in range(5):\n","    spark.table(\"activityCounts\").show()\n","    sleep(1)"]},{"cell_type":"markdown","metadata":{"id":"QPwWmL8m6Ct6"},"source":["## Managing and Monitoring Streaming Queries\n","* [Streaming Query Management](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/query_management.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0djmuzTSqDS-"},"outputs":[],"source":["activityQuery.isActive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"berZpXTsqOO2"},"outputs":[],"source":["activityQuery.status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGUQ6O2iqVpW"},"outputs":[],"source":["activityQuery.lastProgress"]},{"cell_type":"markdown","metadata":{"id":"rm0CSeDbhG_t"},"source":["### Streaming Query Manager\n","\n","* [pyspark.sql.streaming.StreamingQueryManager](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryManager.html)\n","*  [pyspark.sql.SparkSession.streams](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.streams.html):  get the StreamingQueryManager that can be used to manage the currently active queries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8XzVJYRxWrj"},"outputs":[],"source":["spark.streams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ip2JAEmMpNo9"},"outputs":[],"source":["#You can list active streams in your Spark Session:\n","spark.streams.active"]},{"cell_type":"markdown","metadata":{"id":"CCqNKUFriRe-"},"source":["### Streaming Query Listener\n","* [pyspark.sql.streaming.StreamingQueryListener](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQueryListener.html)"]},{"cell_type":"markdown","metadata":{"id":"vtAWaaN1-P4H"},"source":["* [pyspark.sql.DataFrame.observe](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.observe.html)"]},{"cell_type":"markdown","metadata":{"id":"rS9UM3BKzVth"},"source":["## Stop the Streaming Query"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUhLBaxbrN7G"},"outputs":[],"source":["activityQuery.stop()"]},{"cell_type":"code","source":["spark.streams.active"],"metadata":{"id":"uod7nrFYOSDO"},"execution_count":null,"outputs":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3496694700218674,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"1-SparkSQL_DataFrames","widgets":{}},"colab":{"provenance":[{"file_id":"1-3DVy-jpDMwUjlHuvXT5nkt0fWHazHhZ","timestamp":1691337251552}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}