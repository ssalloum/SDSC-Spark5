{"cells":[{"cell_type":"markdown","metadata":{"id":"ICXbyhtyjXZM"},"source":["#**PySpark: DataFrames**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LlkdCTePkOd"},"outputs":[],"source":["# Clone the GitHub repository\n","!git clone https://github.com/ssalloum/SDSC-Spark5.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDt-vQnbPqi1"},"outputs":[],"source":["!ls /content/SDSC-Spark5/data"]},{"cell_type":"markdown","metadata":{"id":"cDefeMsvjXZR"},"source":["# PySpark\n","PySpark is an interface for Apache Spark that allows users to write Spark applications using python APIs. PySpark supports most of Spark’s features such as Spark SQL, Streaming, MLlib (Machine Learning) and Spark Core. For detailed information on these components and APIs, please refer to the [official PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u04pguFNlZvy"},"outputs":[],"source":["#You don't need this on Databricks\n","!pip install pyspark"]},{"cell_type":"markdown","metadata":{"id":"2BTx92K_jXZR"},"source":["## Spark SQL\n","* [Spark SQL](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) is a Spark module for structured data processing.\n","* Spark SQL integrates relational processing (using SQL) and functional programming (using the DataFrame API).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FWxdNXxrjXZS"},"source":["## SparkSession\n","* An essentail class in Spark SQL is [pyspark.sql.SparkSession](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html) which represents a unified entry point to programming in Spark.\n","\n","* In Spark-shell or Databricks notebooks, a SparkSession is created for you, stored in a variable called `spark`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WwCBTK_ljcw"},"outputs":[],"source":["# You don't need this on Databricks or spark-shell\n","from pyspark.sql import SparkSession\n","\n","# Create a Spark Session\n","spark = SparkSession.builder\\\n","        .master(\"local[*]\")\\\n","        .appName(\"Intro to PySpark\")\\\n","        .getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxZdi3iDjXZS"},"outputs":[],"source":["# Check Spark Session Information\n","spark"]},{"cell_type":"markdown","metadata":{"id":"RGS2gYttjXZU"},"source":["## SparkContext\n","* [SparkContext](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html) was the main entry point in earlier versions of Spark.\n","* For working with low-level APIs, [Resilient Distributed Datasets (RDDs)](https://spark.apache.org/docs/latest/rdd-programming-guide.html), and for backward compatibility, you can access SparkContext via SparkSession."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQmpAI-ujXZV"},"outputs":[],"source":["# get SparkContext\n","sc = spark.sparkContext\n","sc"]},{"cell_type":"markdown","metadata":{"id":"1DIcPKJLjXZW"},"source":["# Spark DataFrames\n","\n","\n","*   [pyspark.sql.DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html) represents a distributed collection of data grouped into named columns.\n","\n","* [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html): represents a column in a DataFrame.\n","* [pyspark.sql.Row](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row.html): represents a row in a DataFrame.\n","*   [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html): common functions to work with DataFrames.\n","\n","* A DataFrame can be constructed from a variety of [supported data sources](https://spark.apache.org/docs/latest/sql-data-sources.html).\n"]},{"cell_type":"markdown","metadata":{"id":"x6PQ_t_uaiof"},"source":["## DataFrameReader\n","* [DataFrameReader](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html): loading DataFrames from external sources.\n","* You cannot create an instance of DataFrameReader.\n","* You can access a DataFrameReader through a SparkSession instance using the [read](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html) property to read data from a static data source (streaming data sources has a different method: readStream).\n","* DataFrameReader provides several public methods that can be used with all supported data sources, and may take different arguments for each source: [format](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.format.html), [option](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.option.html), [schema](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.schema.html), and [load](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.load.html).\n","\n","* If you don’t specify the format, then the default is\n","Parquet or whatever is set in 'spark.sql.sources.default'.\n","\n","* DataFrameReader also has methods to directly load data from specific formats/sources such as [parquet](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html), [csv](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html), [json](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html)."]},{"cell_type":"markdown","metadata":{"id":"QkBIwNB7SvaM"},"source":["### Creating DataFrames From CSV files\n","* You can read data from a [CSV file](https://spark.apache.org/docs/latest/sql-data-sources-csv.html) into a DataFrame.\n","* The [pyspark.sql.SparkSession.read](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html) function can be used to read in the CSV file and returns a DataFrame of rows and named columns with the types dictated in the schema. We will use csv files from the flights dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80rCxGpsWFGO"},"outputs":[],"source":["dataPath = \"/content/SDSC-Spark5/data/2015-summary.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9GYMrValVUR"},"outputs":[],"source":["#try with inferSchema\n","flights_df = spark.read\\\n","  .option(\"inferSchema\", \"true\")\\\n","  .option(\"header\", \"true\")\\\n","  .csv(dataPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnYClcJujXZX"},"outputs":[],"source":["flights_df.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GnXDJGrjXZX"},"outputs":[],"source":["flights_df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OV-V0yUV0iKP"},"outputs":[],"source":["flights_df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"UYdWGEmHC0q3"},"source":["## DataFrame Schema\n","* A schema in Spark defines the column names and associated data types for a DataFrame. In addition to inferring the schema from the source data, Spark allows you to define a schema programmatically.\n","\n","* A schema is a [StructType](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructType.html) made up of a number of fields, each field is a [StructField](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructField.htm), that have a name, type, a Boolean flag which specifies whether that column can contain missing or null values, and, finally, users can optionally specify associated metadata with that column.\n","*  Supported data types are defined in [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html).\n","\n","* [Spark SQL Guide: Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pm-iq_EgjXZY"},"outputs":[],"source":["#Define a schema programatically\n","from pyspark.sql.types import *\n","\n","myFlightSchema = StructType([\n","  StructField(\"dest\", StringType(), True),\n","  StructField(\"origin\", StringType(), True),\n","  StructField(\"flights\", LongType(), False)\n","])\n","\n","myFlightSchema"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j97dhRk_YzZ3"},"outputs":[],"source":["flights_df_2015 = spark.read.schema(myFlightSchema).option(\"header\", \"true\").csv(dataPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYT75KqnPg85"},"outputs":[],"source":["flights_df_2015.take(5)"]},{"cell_type":"markdown","metadata":{"id":"H32nISfLvzLV"},"source":["## Columns\n","* [DataFrame.columns](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.columns.html): get all columns names in a DataFrame.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nl9_0lIacbow"},"outputs":[],"source":["flights_df_2015.columns"]},{"cell_type":"markdown","metadata":{"id":"nWWZGAuxnuyI"},"source":["* you can refer to columns in a number of different\n","ways; and you can use them interchangeably: [col()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html), [column()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.column.html), [expr()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgt7SLbxnzkR"},"outputs":[],"source":["from pyspark.sql.functions import expr, col, column\n","\n","flights_df_2015.select(\n","  col(\"dest\"),\n","  column(\"dest\"),\n","  expr(\"lower(dest)\"),\n","  flights_df_2015.dest)\\\n",".show(5)"]},{"cell_type":"markdown","metadata":{"id":"-pKfoXkY1CuM"},"source":["* In Spark DataFrames, Columns are objects represented by [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html) that provides commonly used methods on columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CBrSp4q1M4U"},"outputs":[],"source":["from pyspark.sql import Column\n","\n","flights_df_2015.orderBy(flights_df_2015.flights.desc()).show(5)"]},{"cell_type":"markdown","metadata":{"id":"v9YMW9_Lv2GY"},"source":["## Rows\n","* A row in Spark is an object of [pyspark.sql.Row](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row.html), containing one or more columns.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1h5CTsbM72Ap"},"outputs":[],"source":["#get the first Row\n","flights_df_2015.first()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG8d38oL-j08"},"outputs":[],"source":["#get a list of the first \"num\" of Rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFfX8G30-sUx"},"outputs":[],"source":["flights_df_2015.take(5)"]},{"cell_type":"markdown","metadata":{"id":"RqFnrMZp-fUV"},"source":["* Because Row is an object in Spark and an ordered collection of fields, you can instantiate a Row in each of Spark’s supported languages and access its fields by an index starting at 0:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-uB1JHxOt2p"},"outputs":[],"source":["from pyspark.sql import Row\n","\n","blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n","[\"twitter\", \"LinkedIn\"])\n","\n","# access using index for individual items\n","blog_row[1]\n","'Reynold'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPAkXJ-fFj3m"},"outputs":[],"source":["# the following code results in an array of Row objects.\n","spark.range(5).show()"]},{"cell_type":"markdown","metadata":{"id":"3Dn93q7WPaz8"},"source":["Row objects can be used to create DataFrames if you need them for quick interactivity\n","and exploration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bntpp8YuPcSU"},"outputs":[],"source":["rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n","authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n","authors_df.show()\n","authors_df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"7RbY0UI6YPLv"},"source":["## Parquet Data Source\n","* [Parquet](https://parquet.apache.org/) is an open-source columnar format that offers many I/O\n","optimizations (such as compression, which saves storage space and allows for quick\n","access to data columns).\n","\n","* [Parquet files](https://github.com/apache/parquet-format#file-format) are stored in a directory structure that contains the data files, metadata,\n","a number of compressed files, and some status files.\n","\n","* Spark SQL provides support for [reading and writing Parquet files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html).\n","\n","* Parquet is the default data\n","source in Spark.\n","\n","* Unless you are reading from a streaming data source, there’s no need to supply a\n","schema when reading from a Parquet file, because Parquet saves it as part of its metadata.\n","\n","* Another way to read this same data using the [parquet](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html) method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWsUAIBWldTZ"},"outputs":[],"source":["parquetPath = \"/content/SDSC-Spark5/data/2010-summary.parquet\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRNL7MdUYPLw"},"outputs":[],"source":["df2 = spark.read.parquet(parquetPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQsJXsG3nTPS"},"outputs":[],"source":["df2.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"WmcTB5XralNE"},"source":["## DataFrameWriter\n","* [DataFrameWriter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html) is an interface used to write a DataFrame to external stoage systems.\n","\n","* Unlike with DataFrameReader, you access its instance not from a SparkSession but from the DataFrame you wish to save.\n","\n","* To get an instance handle, use the [DataFrame.write](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html) method for static data sources (DataFrame.writeStream for streaming data sources).\n","\n","* It also provides several public methods: [format](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.format.html), [option](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.option.html), [bucketBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.bucketBy.html), [save](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.save.html), and [saveAsTable](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.saveAsTable.html).\n","* DataFrameWriter also has methods to directly write data to specific formats/sources such as [parquet](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html), [csv](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html), [json](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqZeB5diLn0z"},"outputs":[],"source":["df2.write.parquet(path=\"/tmp/data/df_parquet1\",\n","  mode=\"overwrite\",\n","  compression=\"snappy\")"]},{"cell_type":"markdown","metadata":{"id":"l-6HL_siq6XE"},"source":["#DataFrame Operations: Transformations and Actions\n","\n","* Spark operations on DataFrames can be classified into two types: transformations and actions.\n","* All transformations are evaluated lazily. Their results are not computed immediately,\n","but they are recorded as a lineage. This allows Spark to optimize the execution\n","plan.\n","* Distributed computation occurs upon invoking an action on a DataFrame, e.g.,: `show(), take(), count(), collect()`."]},{"cell_type":"markdown","metadata":{"id":"NDkcHGzHln1_"},"source":["### select\n","The easiest way to work with columns is just to use the [select](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html) method and pass in the column names as strings:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4-uudqsmmDy"},"outputs":[],"source":["flights_df.select(\"DEST_COUNTRY_NAME\").show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjgfTWv9m-o4"},"outputs":[],"source":["flights_df.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show(2)"]},{"cell_type":"markdown","metadata":{"id":"RCpXcZHoo0gc"},"source":["### Adding columns\n","To add a new column to your DataFrame, you can use the [withColumn](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html) method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnEqzDIZpMft"},"outputs":[],"source":["flights_df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",".take(5)"]},{"cell_type":"markdown","metadata":{"id":"DGO9Qq5mp58F"},"source":["### Renaming columns\n","You can rename a column  with the [withColumnRenamed](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html) method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1OA8jrjqSmI"},"outputs":[],"source":["flights_df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"envGS7BvuZw-"},"outputs":[],"source":["# renaming multiple columns\n","flights_df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\")\\\n","  .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"origin\").show(2)"]},{"cell_type":"markdown","metadata":{"id":"wdJ4fEJjp8sn"},"source":["### Removing columns\n","[drop](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.drop.html) is a dedicated method to remove columns from a DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQjJwIIjyaZh"},"outputs":[],"source":["flights_df.drop(\"count\").columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEl8Vz16zkLI"},"outputs":[],"source":["flights_df.drop(\"ORIGIN_COUNTRY_NAME\",\"count\").show(5)"]},{"cell_type":"markdown","metadata":{"id":"mxuu4VxOeL7E"},"source":["### Filtering Rows\n","There are two methods to perform filtering operations: you can use [where](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.where.html) or [filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html)\n","and they both will perform the same operation and accept the same argument types when used\n","with DataFrames. To filter rows, you need an expression that evaluates to true or false."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqOEwTWb0Cuf"},"outputs":[],"source":["flights_df.filter(col(\"count\") < 2).show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlgpafgU14kN"},"outputs":[],"source":["flights_df.where(\"count < 2\").show(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qfyk5lg3R6z"},"outputs":[],"source":["flights_df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Singapore\")\\\n",".show(2)"]},{"cell_type":"markdown","metadata":{"id":"R6WFGhJUeaW4"},"source":["### Getting Unique Rows\n","To extract the unique or distinct values in a DataFrame, you can use the [distinct](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html?highlight=distinct#pyspark.sql.DataFrame.distinct) method on a\n","DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGK58YPC4Tja"},"outputs":[],"source":["flights_df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ja-q7QEd5fcJ"},"outputs":[],"source":["flights_df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"]},{"cell_type":"markdown","metadata":{"id":"dcv4fmAkegK2"},"source":["### Random Samples\n","To sample some random records from your DataFrame, use the [sample](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sample.html) method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0YcaJcE6jHf"},"outputs":[],"source":["flights_df.sample(withReplacement = False,\n","                      fraction= 0.5,\n","                      seed = 5).count()"]},{"cell_type":"markdown","metadata":{"id":"-eAiauqNe0qY"},"source":["### Sorting Rows\n","\n","There are two equivalent operations to sort the values in a DataFrame: [sort](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sort.html) and [orderBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.orderBy.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuEY7GSXCgPh"},"outputs":[],"source":["flights_df.sort(\"count\", ascending=False).show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GZxK726Cn_P"},"outputs":[],"source":["flights_df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGhto_vKCqGO"},"outputs":[],"source":["flights_df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\"), ascending=False).show(5)"]},{"cell_type":"markdown","metadata":{"id":"FGZdgNbqjXZs"},"source":["Let’s find the top five destination countries in the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvvsFAKAjXZs"},"outputs":[],"source":["from pyspark.sql.functions import desc\n","\n","flights_df\\\n","  .groupBy(\"DEST_COUNTRY_NAME\")\\\n","  .sum(\"count\")\\\n","  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n","  .sort(desc(\"destination_total\"))\\\n","  .limit(5)\\\n","  .show()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3496694700218674,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"1-SparkSQL_DataFrames","widgets":{}},"colab":{"provenance":[{"file_id":"1-3DVy-jpDMwUjlHuvXT5nkt0fWHazHhZ","timestamp":1691337251552}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}